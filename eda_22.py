# -*- coding: utf-8 -*-
"""EDA 22.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iRzcgdeQ5W_HrL_FCV-qufKAX295Vu95
"""

import pandas as pd
df = pd.read_csv('adult_with_headers.csv')
df.head()

df.columns

df.info()

df.describe()

df.isnull().sum()

from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder

num_col = df.select_dtypes(include=['float', 'int']).columns  # Numerical columns

cat_col = df.select_dtypes(include=['object']).columns  # Categorical Columns

# Standard Scaling
SS = StandardScaler()
df[num_col] = SS.fit_transform(df[num_col])
df.head()

# Min-Max Scaling
MM = MinMaxScaler()
df[num_col] = MM.fit_transform(df[num_col])
df.head()

# Apply One-Hot Encoding

OHE = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
dummy = OHE.fit_transform(df[cat_col])
#print(dummy)
# Create a DataFrame from the encoded data
df_encoded = pd.DataFrame(dummy, columns=OHE.get_feature_names_out(cat_col))

df_final = pd.concat([df[num_col], df_encoded], axis=1)
df_final.head()

from sklearn.preprocessing import LabelEncoder
LE = LabelEncoder()

# Select categorical columns with more than 5 categories
categories = [col for col in df.select_dtypes(include=['object']).columns if df[col].nunique() > 5]


# Apply Label Encoding
for col in categories:
    df_final[col] = LE.fit_transform(df[col])

df_final.head()

# 1. Education workclass Feature:

df_final['education_workclass'] = df['education'] + '_' + df['workclass']
df_final['education_workclass'] = LE.fit_transform(df_final['education_workclass'])

# 2. Age Squared Feature:

df_final['age_squared'] = df['age'] ** 2
df_final

import numpy as np

# Check for skewness in numerical features
skewed = df[num_col].skew()
skewed

# Apply log transformation to 'capital-gain' (assuming it's positively skewed)
df_final['capital-gain_log'] = np.log1p(df['capital_gain'])  # Use log1p to handle zero values

df_final

from sklearn.ensemble import IsolationForest

# Create an Isolation Forest model
ISO = IsolationForest(contamination=0.05)  # Adjust contamination as needed

# Fit the model to your data
ISO.fit(df_final)

# Get outlier predictions (-1 for outliers, 1 for inliers)
outlier_predictions = ISO.predict(df_final)

# Filter out outliers
df_no_outliers = df_final[outlier_predictions == 1]

df_no_outliers

#pip install ppscore

import ppscore as pps

# Compute the PPS matrix
pps_matrix = pps.matrix(df_no_outliers)
pps_matrix = pps_matrix.pivot(columns='x', index='y', values='ppscore')

print("Predictive Power Score (PPS) Matrix:\n", pps_matrix)



# Compute the Correlation Matrix
correlation_matrix = df_no_outliers.corr()

print("Correlation Matrix:\n", correlation_matrix)

high_pps_pairs = pps_matrix[pps_matrix > 0.2].stack().reset_index()
high_pps_pairs.columns = ['Feature 1', 'Feature 2', 'PPS']

print("Pairs of features with high PPS scores:\n", high_pps_pairs)

# Compare high PPS pairs with correlation matrix
for _, row in high_pps_pairs.iterrows():
  feature1 = row['Feature 1']
  feature2 = row['Feature 2']
  pps_value = row['PPS']
  corr_value = correlation_matrix.loc[feature1, feature2]
  print(f"Features: {feature1} and {feature2}")
  print(f"PPS: {pps_value:.3f}, Correlation: {corr_value:.3f}")
  if abs(pps_value - abs(corr_value)) > 0.2:
    print("  -> Significant difference between PPS and correlation.")
  print()

