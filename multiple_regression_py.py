# -*- coding: utf-8 -*-
"""Multiple-Regression.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FlEdQbn6hshuLWDzxo6wLRHgEQwiswL3

# Import Data File and Cleaning
"""

import numpy as np
import pandas as pd
df = pd.read_csv("ToyotaCorolla - MLR.csv")
df.head()

df.info()

df.isnull().sum()

df.describe()

num_col = ['Price', 'Age_08_04', 'KM', 'HP', 'cc', 'Doors', 'Gears', 'Weight']
num_col

cat_col = ['Fuel_Type']
cat_col

import matplotlib.pyplot as plt
import seaborn as sns

# Bar Graph
for i in num_col:
    plt.figure()
    sns.histplot(df[i])
    plt.title(f'Distribution of {i}')
    plt.show()

# Scatter Plot
for i in num_col:
    plt.figure()
    sns.scatterplot(x = i, y = 'Price', data = df)
    plt.title(f'{i} vs Price')
    plt.show()

# Box plot
for i in cat_col:
    plt.figure()
    sns.boxplot(x = i, y = 'Price', data = df)
    plt.title(f'{i} vs Price')
    plt.show()

from sklearn.preprocessing import LabelEncoder, StandardScaler

# Categorical variables
LE = LabelEncoder()
df['Fuel_Type'] = LE.fit_transform(df['Fuel_Type'])
df
df['Age_08_04_KM'] = df['Age_08_04'] * df['KM']

# Numerical variables

SS = StandardScaler()
df[num_col] = SS.fit_transform(df[num_col])
df

X = df.drop('Price', axis=1)
Y = df['Price']

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Model 1
model1 = LinearRegression()
model1.fit(X_train, Y_train)
Y_pred1 = model1.predict(X_test)

# Model 2
model2 = LinearRegression()
model2.fit(X_train, Y_train)
Y_pred2 = model2.predict(X_test)

# Model 3
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

model3 = LinearRegression()
model3.fit(X_train_poly, Y_train)
Y_pred3 = model3.predict(X_test_poly)

# Evaluate Model 1
mse1 = mean_squared_error(Y_test, Y_pred1)
print("Model 1 - All Features:")
print("Mean Squared Error:", mse1.round(3))
print("Root Mean Squared Error:", np.sqrt(mse1).round(3))
print("R-squared:", r2_score(Y_test, Y_pred1).round(3))
print(pd.DataFrame(model1.coef_, X_train.columns, columns=['Coefficient']))

# Evaluate Model 2
mse2 = mean_squared_error(Y_test, Y_pred2)
print("Model 2 - With Interaction Term (Age_08_04 * KM):")
print("Mean Squared Error:", mse2.round(3))
print("Root Mean Squared Error:", np.sqrt(mse2).round(3))
print("R-squared:", r2_score(Y_test, Y_pred2).round(3))
print(pd.DataFrame(model2.coef_, X_train.columns, columns=['Coefficient']))

# Evaluate Model 3
mse3 = mean_squared_error(Y_test, Y_pred3)
print("Model 3 - Polynomial Regression (Degree 2):")
print("Mean Squared Error:", mse3.round(3))
print("Root Mean Squared Error:", np.sqrt(mse3).round(3))
print("R-squared:", r2_score(Y_test, Y_pred3).round(3))
print(pd.DataFrame(model3.coef_, poly.get_feature_names_out(X_train.columns), columns=['Coefficient']))

# Lasso Regression
from sklearn.linear_model import Lasso

lasso = Lasso(alpha=0.1)
lasso.fit(X_train, Y_train)
Y_pred_lasso = lasso.predict(X_test)

print('Lasso Regression: ')
print('MSE:', mean_squared_error(Y_test, Y_pred_lasso))
print('R-squared:', r2_score(Y_test, Y_pred_lasso))

# Ridge Regression
from sklearn.linear_model import Ridge

ridge = Ridge(alpha=0.1)
ridge.fit(X_train, Y_train)
Y_pred_ridge = ridge.predict(X_test)

print('Ridge Regression: ')
print('MSE:', mean_squared_error(Y_test, Y_pred_ridge))
print('R-squared:', r2_score(Y_test, Y_pred_ridge))

# Linearity Check
for column in X_train.columns:
    plt.scatter(X_train[column], Y_train)
    plt.title(f'Relationship between {column} and Price')
    plt.xlabel(column)
    plt.ylabel('Price')
    plt.show()

# Calculate residuals
residuals = Y_train - model1.predict(X_train)

# Residual Plot
plt.scatter(model1.predict(X_train), residuals)
plt.axhline(y=0, color='r', linestyle='--')
plt.title('Residuals vs Predicted Values')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.show()

import statsmodels.api as sm

# Breusch-Pagan test
bp_test = sm.stats.diagnostic.het_breuschpagan(residuals, X_train)
bp_test_labels = ['Lagrange multiplier statistic', 'p-value', 'f-value', 'f p-value']
bp_test_results = dict(zip(bp_test_labels, bp_test))
bp_test_results

# Q-Q Plot
sm.qqplot(residuals, line='s')
plt.title('Q-Q Plot of Residuals')
plt.show()

from scipy import stats

# Shapiro-Wilk Test
shapiro_test = stats.shapiro(residuals)
shapiro_test

